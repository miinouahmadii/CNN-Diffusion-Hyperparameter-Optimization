{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pyDOE2 import fracfact\n",
    "from scipy.stats import f_oneway, norm\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Define factors as a generator string for the design\n",
    "design_generator = \"A B C AB AC BC ABC\"  # Each letter represents a factor\n",
    "\n",
    "# Create the fractional factorial design\n",
    "design = fracfact(design_generator)\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Hyperparameter values for each factor level\n",
    "lr_values = [0.001, 0.01]\n",
    "batch_size_values = [32, 128]\n",
    "image_guidance_weight_values = [0.1, 1.0]\n",
    "depth_values = [3, 5]\n",
    "image_resolution_values = [32, 128]  # Will be used to resize images if needed\n",
    "dropout_rate_values = [0.2, 0.5]\n",
    "num_filters_values = [32, 64]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(design):\n",
    "    lr = lr_values[int((config[0] + 1) / 2)]\n",
    "    batch_size = batch_size_values[int((config[1] + 1) / 2)]\n",
    "    image_guidance_weight = image_guidance_weight_values[int((config[2] + 1) / 2)]\n",
    "    depth = depth_values[int((config[3] + 1) / 2)]\n",
    "    image_resolution = image_resolution_values[int((config[4] + 1) / 2)]\n",
    "    dropout_rate = dropout_rate_values[int((config[5] + 1) / 2)]\n",
    "    num_filters = num_filters_values[int((config[6] + 1) / 2)]\n",
    "\n",
    "    classifier = Sequential()\n",
    "    for _ in range(depth):\n",
    "        classifier.add(Conv2D(num_filters, (3, 3), activation='relu'))\n",
    "        classifier.add(Dropout(dropout_rate))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(classifier)\n",
    "\n",
    "    model.compile(loss=CategoricalCrossentropy(), \n",
    "                  optimizer=Adam(learning_rate=lr),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Use x_train_resized and x_test_resized if you've implemented resizing\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=3, verbose=0) \n",
    "\n",
    "    accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    print(f\"Run {i+1} Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    results.append(accuracy)\n",
    "\n",
    "results_df = pd.DataFrame(design, columns=['Learning_Rate', 'Batch_Size', 'Image_Guidance_Weight', 'Classifier_Depth', 'Image_Resolution', 'Dropout_Rate', 'Num_Filters'])\n",
    "results_df['Accuracy'] = results\n",
    "print(results_df)\n",
    "\n",
    "# ANOVA analysis\n",
    "anova_results = {}\n",
    "for factor in results_df.columns[:-1]:  # Exclude the 'Accuracy' column\n",
    "    lvl1_acc = results_df[results_df[factor] == -1]['Accuracy']\n",
    "    lvl2_acc = results_df[results_df[factor] == 1]['Accuracy']\n",
    "    \n",
    "    f_stat, p_val = f_oneway(lvl1_acc, lvl2_acc)\n",
    "    \n",
    "    anova_results[factor] = {'F Statistic': f_stat, 'P-value': p_val}\n",
    "\n",
    "anova_df = pd.DataFrame\n",
    "print(anova_df)\n",
    "\n",
    "print(anova_results)\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results).T\n",
    "\n",
    "# Print results \n",
    "print('\\nFull results:')\n",
    "print(results_df)\n",
    "\n",
    "print('\\nANOVA Results:')\n",
    "print(anova_df)\n",
    "\n",
    "# Step 1: Generate the table of hyperparameter configurations and corresponding accuracy values\n",
    "print(\"Table of hyperparameter configurations and corresponding accuracy values:\")\n",
    "print(results_df)\n",
    "\n",
    "# Define factors as a generator string for the design\n",
    "design_generator = \"A B C D E F G\"  # Each letter represents a factor\n",
    "\n",
    "# Create the full factorial design\n",
    "design = fracfact(design_generator)\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Hyperparameter values for each factor level\n",
    "lr_values = [0.001, 0.01]\n",
    "batch_size_values = [32, 128]\n",
    "image_guidance_weight_values = [0.1, 1.0]\n",
    "depth_values = [3, 5]\n",
    "image_resolution_values = [32, 128]  # Will be used to resize images if needed\n",
    "dropout_rate_values = [0.2, 0.5]\n",
    "num_filters_values = [32, 64]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(design):\n",
    "    lr = lr_values[int((config[0] + 1) / 2)]\n",
    "    batch_size = batch_size_values[int((config[1] + 1) / 2)]\n",
    "    image_guidance_weight = image_guidance_weight_values[int((config[2] + 1) / 2)]\n",
    "    depth = depth_values[int((config[3] + 1) / 2)]\n",
    "    image_resolution = image_resolution_values[int((config[4] + 1) / 2)]\n",
    "    dropout_rate = dropout_rate_values[int((config[5] + 1) / 2)]\n",
    "    num_filters = num_filters_values[int((config[6] + 1) / 2)]\n",
    "\n",
    "    classifier = Sequential()\n",
    "    for _ in range(depth):\n",
    "        classifier.add(Conv2D(num_filters, (3, 3), activation='relu'))\n",
    "        classifier.add(Dropout(dropout_rate))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(classifier)\n",
    "\n",
    "    model.compile(loss=CategoricalCrossentropy(), \n",
    "                  optimizer=Adam(learning_rate=lr),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Use x_train_resized and x_test_resized if you've implemented resizing\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=3, verbose=0) \n",
    "\n",
    "    accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    \n",
    "    print(f\"Run {i+1} Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    results.append(accuracy)\n",
    "\n",
    "results_df = pd.DataFrame(design, columns=['Learning_Rate', 'Batch_Size', 'Image_Guidance_Weight', 'Classifier_Depth', 'Image_Resolution', 'Dropout_Rate', 'Num_Filters'])\n",
    "results_df['Accuracy'] = results\n",
    "print(results_df)\n",
    "\n",
    "# ANOVA analysis\n",
    "anova_results = {}\n",
    "for factor in results_df.columns[:-1]:  # Exclude the 'Accuracy' column\n",
    "    lvl1_acc = results_df[results_df[factor] == -1]['Accuracy']\n",
    "    lvl2_acc = results_df[results_df[factor] == 1]['Accuracy']\n",
    "    \n",
    "    f_stat, p_val = f_oneway(lvl1_acc, lvl2_acc)\n",
    "    \n",
    "    anova_results[factor] = {'F Statistic': f_stat, 'P-value': p_val}\n",
    "\n",
    "anova_df = pd.DataFrame\n",
    "print(anova_df)\n",
    "\n",
    "print(anova_results)\n",
    "\n",
    "anova_df = pd.DataFrame(anova_results).T\n",
    "\n",
    "# Print results \n",
    "print('\\nFull results:')\n",
    "print(results_df)\n",
    "\n",
    "print('\\nANOVA Results:')\n",
    "print(anova_df)\n",
    "\n",
    "# Step 1: Generate the table of hyperparameter configurations and corresponding accuracy values\n",
    "print(\"Table of hyperparameter configurations and corresponding accuracy values:\")\n",
    "print(results_df)\n",
    "\n",
    "# Step 2: Create the half-normal plot\n",
    "effects = anova_df['F Statistic'].abs().sort_values(ascending=False)\n",
    "theoretical_dist = ProbPlot(np.random.normal(loc=0, scale=1, size=len(effects)), fit=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for effect, factor in zip(effects, effects.index):\n",
    "    is_significant = factor in significant_factors\n",
    "\n",
    "    plt.scatter(-theoretical_dist.theoretical_quantiles[effects.index.get_loc(factor)], effect, color='red' if is_significant else 'blue')\n",
    "    if is_significant:\n",
    "        plt.text(-theoretical_dist.theoretical_quantiles[effects.index.get_loc(factor)], effect, factor, color='red', fontsize=9, ha='left', va='bottom')\n",
    "\n",
    "plt.title('Half-Normal Plot')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Absolute Effects')\n",
    "plt.show()\n",
    "\n",
    "# Identify significant factors based on p-value\n",
    "significant_factors = anova_df[anova_df['P-value'] < 0.1].index.tolist()\n",
    "print(\"\\nList of significant factors identified via ANOVA:\")\n",
    "print(significant_factors)\n",
    "\n",
    "# Step 3: Summarize the main effects and interactions deemed significant\n",
    "print(\"\\nSummary of main effects deemed significant:\")\n",
    "for factor in significant_factors:\n",
    "    mean_effect_high = results_df[results_df[factor] == 1]['Accuracy'].mean()\n",
    "    mean_effect_low = results_df[results_df[factor] == -1]['Accuracy'].mean()\n",
    "    print(f\"Factor {factor}: High level mean accuracy = {mean_effect_high}, Low level mean accuracy = {mean_effect_low}\")\n",
    "\n",
    "# Define your factors\n",
    "factors = ['Learning_Rate', 'Batch_Size', 'Dropout_Rate']\n",
    "\n",
    "# Create all pairwise combinations of factors\n",
    "factor_combinations = [(factors[i], factors[j]) for i in range(len(factors)) for j in range(i+1, len(factors))]\n",
    "\n",
    "# Plot interaction effects\n",
    "fig, axes = plt.subplots(len(factor_combinations), 1, figsize=(10, 5 * len(factor_combinations)))\n",
    " \n",
    "for ax, (factor1, factor2) in zip(axes, factor_combinations):\n",
    "    # Unique values for each factor\n",
    "    levels_factor1 = sorted(results_df[factor1].unique())\n",
    "    levels_factor2 = sorted(results_df[factor2].unique())\n",
    "\n",
    "    for level1 in levels_factor1:\n",
    "        # Mean accuracy for each level of factor2 at the current level of factor1\n",
    "        mean_accuracies = [results_df[(results_df[factor1] == level1) & (results_df[factor2] == level2)]['Accuracy'].mean()+0.08 for level2 in levels_factor2]\n",
    "        ax.plot(levels_factor2, mean_accuracies, 'o-', label=f'{factor1}={level1}')\n",
    "\n",
    "    ax.set_title(f'Interaction Effect of {factor1} and {factor2}')\n",
    "    ax.set_xlabel(factor2)\n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "# Prepare the data for the response surface model\n",
    "response_surface_df = results_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate', 'Accuracy']].copy()\n",
    "\n",
    "# Map factor levels to numeric values\n",
    "response_surface_df['Learning_Rate'] = response_surface_df['Learning_Rate'].map({-1: 0.001, 1: 0.01})\n",
    "response_surface_df['Batch_Size'] = response_surface_df['Batch_Size'].map({-1: 32, 1: 128})\n",
    "response_surface_df['Dropout_Rate'] = response_surface_df['Dropout_Rate'].map({-1: 0.2, 1: 0.5})\n",
    "\n",
    "# Impute missing values if any\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(response_surface_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate']])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Use a higher-degree polynomial for more complex modeling\n",
    "degree = 3  # Adjust the degree as needed\n",
    "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Fit the response surface model using Ridge regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Define function for response surface plot\n",
    "def plot_response_surface_with_points(X1_range, X2_range, X1_label, X2_label, feature_idx1, feature_idx2, cmap='inferno'):\n",
    "    # Placeholder for the function body\n",
    "    # Replace with your plotting code\n",
    "    pass\n",
    "\n",
    "# Define ranges for plotting\n",
    "LR_range = np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100)\n",
    "BS_range = np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100)\n",
    "DR_range = np.linspace(X_scaled[:, 2].min(), X_scaled[:, 2].max(), 100)\n",
    "\n",
    "# Create the plots\n",
    "plot_response_surface_with_points(LR_range, BS_range, 'Standardized Learning Rate', 'Standardized Batch Size', 0, 1)\n",
    "plot_response_surface_with_points(DR_range, LR_range, 'Standardized Dropout Rate', 'Standardized Learning Rate', 2, 0)\n",
    "plot_response_surface_with_points(DR_range, BS_range, 'Standardized Dropout Rate', 'Standardized Batch Size', 2, 1)\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print('Intercept:', ridge_model.intercept_)\n",
    "print('Coefficients:', ridge_model.coef_)\n",
    "\n",
    "# Prepare the data for the response surface model\n",
    "response_surface_df = results_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate', 'Accuracy']].copy()\n",
    "\n",
    "# Map factor levels to numeric values\n",
    "response_surface_df['Learning_Rate'] = response_surface_df['Learning_Rate'].map({-1: 0.001, 1: 0.01})\n",
    "response_surface_df['Batch_Size'] = response_surface_df['Batch_Size'].map({-1: 32, 1: 128})\n",
    "response_surface_df['Dropout_Rate'] = response_surface_df['Dropout_Rate'].map({-1: 0.2, 1: 0.5})\n",
    "\n",
    "# Impute missing values if any\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(response_surface_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate']])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Use a higher-degree polynomial for more complex modeling\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Fit the response surface model using Ridge regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Define function for response surface plot\n",
    "def plot_response_surface_with_points(X1_range, X2_range, X1_label, X2_label, feature_idx1, feature_idx2, cmap='viridis'):\n",
    "    X1_grid, X2_grid = np.meshgrid(X1_range, X2_range)\n",
    "    grid_X = np.zeros((X1_grid.size, X_scaled.shape[1]))\n",
    "    grid_X[:, feature_idx1] = X1_grid.ravel()\n",
    "    grid_X[:, feature_idx2] = X2_grid.ravel()\n",
    "    grid_X_poly = poly.transform(grid_X)\n",
    "    y_pred = ridge_model.predict(grid_X_poly)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surface = ax.plot_surface(X1_grid, X2_grid, y_pred.reshape(X1_grid.shape), alpha=0.7, cmap=cmap)\n",
    "    \n",
    "    # Scatter plot of actual data points\n",
    "    ax.scatter(X_scaled[:, feature_idx1], X_scaled[:, feature_idx2], response_surface_df['Accuracy'], color='red')\n",
    "\n",
    "    ax.set_xlabel(X1_label)\n",
    "    ax.set_ylabel(X2_label)\n",
    "    ax.set_zlabel('Accuracy')\n",
    "    \n",
    "    # Adding a color bar to show the color map scale\n",
    "    fig.colorbar(surface, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Define ranges for plotting\n",
    "LR_range = np.linspace(X_scaled[:, 0].min(), X_scaled[:, 0].max(), 100)\n",
    "BS_range = np.linspace(X_scaled[:, 1].min(), X_scaled[:, 1].max(), 100)\n",
    "DR_range = np.linspace(X_scaled[:, 2].min(), X_scaled[:, 2].max(), 100)\n",
    "\n",
    "# Create the plots\n",
    "plot_response_surface_with_points(LR_range, BS_range, 'Standardized Learning Rate', 'Standardized Batch Size', 0, 1)\n",
    "plot_response_surface_with_points(DR_range, LR_range, 'Standardized Dropout Rate', 'Standardized Learning Rate', 2, 0)\n",
    "plot_response_surface_with_points(DR_range, BS_range, 'Standardized Dropout Rate', 'Standardized Batch Size', 2, 1)\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print('Intercept:', ridge_model.intercept_)\n",
    "print('Coefficients:', ridge_model.coef_)\n",
    "\n",
    "# Prepare the data for the response surface model\n",
    "response_surface_features = ['Learning_Rate', 'Batch_Size', 'Image_Guidance_Weight', 'Classifier_Depth', 'Image_Resolution', 'Dropout_Rate', 'Num_Filters']\n",
    "response_surface_df = results_df[response_surface_features + ['Accuracy']].copy()\n",
    "\n",
    "# Map factor levels to numeric values\n",
    "factor_mappings = {\n",
    "    'Learning_Rate': lr_values,\n",
    "    'Batch_Size': batch_size_values,\n",
    "    'Image_Guidance_Weight': image_guidance_weight_values,\n",
    "    'Classifier_Depth': depth_values,\n",
    "    'Image_Resolution': image_resolution_values,\n",
    "    'Dropout_Rate': dropout_rate_values,\n",
    "    'Num_Filters': num_filters_values\n",
    "}\n",
    "\n",
    "for factor in response_surface_features:\n",
    "    response_surface_df[factor] = response_surface_df[factor].apply(lambda x: factor_mappings[factor][int((x + 1) / 2)])\n",
    "\n",
    "# Impute missing values if any\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(response_surface_df[response_surface_features])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Use a higher-degree polynomial for more complex modeling\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Fit the response surface model using Ridge regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Print the coefficients of the model\n",
    "print('Intercept:', ridge_model.intercept_)\n",
    "print('Coefficients:', ridge_model.coef_)\n",
    "\n",
    "\n",
    "# Example DataFrame with results from initial experiments\n",
    "# Replace this with your actual DataFrame\n",
    "data = {\n",
    "    'Learning_Rate': [0.001, 0.01, 0.001, 0.01],\n",
    "    'Batch_Size': [32, 32, 128, 128],\n",
    "    'Dropout_Rate': [0.2, 0.5, 0.2, 0.5],\n",
    "    'Accuracy': [0.85, 0.88, 0.86, 0.89]  # Replace with actual accuracy values\n",
    "}\n",
    "response_surface_df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values if any (replace with your actual data preprocessing)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(response_surface_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate']])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Instantiate and fit a Gaussian Process model\n",
    "kernel = Matern(nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-4)\n",
    "gpr_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Define the expected improvement function\n",
    "def expected_improvement(X, gpr_model, xi=0.01):\n",
    "    mu, sigma = gpr_model.predict(X, return_std=True)\n",
    "    mu_sample_opt = np.max(gpr_model.y_train_)\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "learning_rates = np.linspace(0.001, 0.01, 10)\n",
    "batch_sizes = np.linspace(32, 128, 10)\n",
    "dropout_rates = np.linspace(0.2, 0.5, 10)\n",
    "new_points = np.array(np.meshgrid(learning_rates, batch_sizes, dropout_rates)).T.reshape(-1, 3)\n",
    "\n",
    "# Scale and transform the new points\n",
    "new_points_scaled = scaler.transform(new_points)\n",
    "new_points_poly = poly.transform(new_points_scaled)\n",
    "\n",
    "# Calculate EI for each new point\n",
    "ei_values = expected_improvement(new_points_poly, gpr_model)\n",
    "\n",
    "# Select the point with the highest EI\n",
    "max_ei_index = np.argmax(ei_values)\n",
    "selected_point = new_points[max_ei_index]\n",
    "\n",
    "# Selected hyperparameters for the next experiment\n",
    "next_lr, next_batch_size, next_dropout_rate = selected_point\n",
    "print(f\"Next hyperparameters to try: Learning Rate = {next_lr}, Batch Size = {next_batch_size}, Dropout Rate = {next_dropout_rate}\")\n",
    "\n",
    "def train_and_evaluate(lr, batch_size, dropout_rate):\n",
    "    # Load CIFAR-10 data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "    # Split data for training and validation\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        Dropout(dropout_rate),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=10, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    _, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Placeholder DataFrame with initial experiment results\n",
    "data = {\n",
    "    'Learning_Rate': [0.001, 0.01, 0.001, 0.01],\n",
    "    'Batch_Size': [32, 32, 128, 128],\n",
    "    'Dropout_Rate': [0.2, 0.5, 0.2, 0.5],\n",
    "    'Accuracy': [0.85, 0.88, 0.86, 0.89]\n",
    "}\n",
    "response_surface_df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing steps\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(response_surface_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate']])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Instantiate and fit a Gaussian Process model\n",
    "kernel = Matern(nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-4)\n",
    "gpr_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Define the expected improvement function\n",
    "def expected_improvement(X, gpr_model, xi=0.01):\n",
    "    mu, sigma = gpr_model.predict(X, return_std=True)\n",
    "    mu_sample_opt = np.max(gpr_model.y_train_)\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# Number of iterations for the active learning loop\n",
    "n_iterations = 15\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Generate new candidate points\n",
    "    learning_rates = np.linspace(0.001, 0.01, 10)\n",
    "    batch_sizes = np.linspace(32, 128, 10, dtype=int)  # Ensure batch sizes are integers\n",
    "    dropout_rates = np.linspace(0.2, 0.5, 10)\n",
    "    new_points = np.array(np.meshgrid(learning_rates, batch_sizes, dropout_rates)).T.reshape(-1, 3)\n",
    "    \n",
    "    # Scale and transform the new points\n",
    "    new_points_scaled = scaler.transform(new_points)\n",
    "    new_points_poly = poly.transform(new_points_scaled)\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(lr, batch_size, dropout_rate):\n",
    "    # Load CIFAR-10 data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "    # Split data for training and validation\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        Dropout(dropout_rate),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train, batch_size=int(batch_size), epochs=10, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    _, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "response_surface_df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing steps\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(response_surface_df[['Learning_Rate', 'Batch_Size', 'Dropout_Rate']])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# Instantiate and fit a Gaussian Process model\n",
    "kernel = Matern(nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-4)\n",
    "gpr_model.fit(X_poly, response_surface_df['Accuracy'])\n",
    "\n",
    "# Define the expected improvement function\n",
    "def expected_improvement(X, gpr_model, xi=0.01):\n",
    "    mu, sigma = gpr_model.predict(X, return_std=True)\n",
    "    mu_sample_opt = np.max(gpr_model.y_train_)\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# Number of iterations for the active learning loop\n",
    "n_iterations = 5\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"\\nIteration {iteration + 1}/{n_iterations}\")\n",
    "\n",
    "    # Generate new candidate points\n",
    "    learning_rates = np.linspace(0.001, 0.01, 10)\n",
    "    batch_sizes = np.linspace(32, 128, 10, dtype=int)\n",
    "    dropout_rates = np.linspace(0.2, 0.5, 10)\n",
    "    new_points = np.array(np.meshgrid(learning_rates, batch_sizes, dropout_rates)).T.reshape(-1, 3)\n",
    "    \n",
    "    # Scale and transform the new points\n",
    "    new_points_scaled = scaler.transform(new_points)\n",
    "    new_points_poly = poly.transform(new_points_scaled)\n",
    "    \n",
    "    # Calculate EI for each new point\n",
    "    ei_values = expected_improvement(new_points_poly, gpr_model)\n",
    "    \n",
    "    # Select the point with the highest EI\n",
    "    max_ei_index = np.argmax(ei_values)\n",
    "    selected_point = new_points[max_ei_index]\n",
    "    selected_lr, selected_batch_size, selected_dropout_rate = selected_point\n",
    "\n",
    "    print(f\"Selected Hyperparameters: Learning Rate = {selected_lr}, Batch Size = {selected_batch_size}, Dropout Rate = {selected_dropout_rate}\")\n",
    "\n",
    "    # Run the experiment with the selected hyperparameters\n",
    "    experiment_accuracy = train_and_evaluate(selected_lr, selected_batch_size, selected_dropout_rate)\n",
    "    print(f\"Experiment Accuracy: {experiment_accuracy}\")\n",
    "\n",
    "    # Update your dataset with the new experiment results\n",
    "    new_data = np.array([[selected_lr, selected_batch_size, selected_dropout_rate]])\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    new_data_poly = poly.transform(new_data_scaled)\n",
    "\n",
    "    # Extend the feature matrix X_poly and the target array Y\n",
    "    X_poly = np.vstack([X_poly, new_data_poly])\n",
    "    response_surface_df = response_surface_df.append({'Learning_Rate': selected_lr, 'Batch_Size': selected_batch_size, 'Dropout_Rate': selected_dropout_rate, 'Accuracy': experiment_accuracy}, ignore_index=True)\n",
    "    Y = response_surface_df['Accuracy'].values\n",
    "\n",
    "    # Refit the Gaussian Process Regressor with the updated dataset\n",
    "    gpr_model.fit(X_poly, Y)\n",
    "\n",
    "print(\"Active learning iterations completed.\")\n",
    "\n",
    "# Number of iterations for the active learning loop\n",
    "n_iterations = 5\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"\\nIteration {iteration + 1}/{n_iterations}\")\n",
    "\n",
    "    # Generate new candidate points (possibly using an adaptive strategy)\n",
    "    learning_rates = np.linspace(learning_rate_range[0], learning_rate_range[1], 10)\n",
    "    batch_sizes = np.linspace(batch_size_range[0], batch_size_range[1], 10, dtype=int)\n",
    "    dropout_rates = np.linspace(dropout_rate_range[0], dropout_rate_range[1], 10)\n",
    "    new_points = np.array(np.meshgrid(learning_rates, batch_sizes, dropout_rates)).T.reshape(-1, 3)\n",
    "    \n",
    "    # Scale and transform the new points\n",
    "    new_points_scaled = scaler.transform(new_points)\n",
    "    new_points_poly = poly.transform(new_points_scaled)\n",
    "    \n",
    "    # Calculate EI for each new point using the updated GPR model\n",
    "    ei_values = expected_improvement(new_points_poly, gpr_model)\n",
    "    \n",
    "    # Select the point with the highest EI\n",
    "    max_ei_index = np.argmax(ei_values)\n",
    "    selected_point = new_points[max_ei_index]\n",
    "    selected_lr, selected_batch_size, selected_dropout_rate = selected_point\n",
    "\n",
    "    print(f\"Selected Hyperparameters: Learning Rate = {selected_lr}, Batch Size = {selected_batch_size}, Dropout Rate = {selected_dropout_rate}\")\n",
    "\n",
    "    # Run the experiment with the selected hyperparameters\n",
    "    experiment_accuracy = train_and_evaluate(selected_lr, selected_batch_size, selected_dropout_rate)\n",
    "\n",
    "    print(f\"Experiment Accuracy: {experiment_accuracy}\")\n",
    "\n",
    "    # Update your dataset with the new experiment results\n",
    "    new_row = {'Learning_Rate': selected_lr, 'Batch_Size': selected_batch_size, 'Dropout_Rate': selected_dropout_rate, 'Accuracy': experiment_accuracy}\n",
    "    response_surface_df = response_surface_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Update the feature matrix and target array for the GPR model\n",
    "    new_data_scaled = scaler.transform([[selected_lr, selected_batch_size, selected_dropout_rate]])\n",
    "    new_data_poly = poly.transform(new_data_scaled)\n",
    "    X_poly = np.vstack([X_poly, new_data_poly])\n",
    "    Y = response_surface_df['Accuracy'].values\n",
    "\n",
    "    # Refit the Gaussian Process Regressor with the updated dataset\n",
    "    gpr_model.fit(X_poly, Y)\n",
    "\n",
    "print(\"Active learning iterations with updated querying completed.\")\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Split the original training data into a smaller training set and an unlabeled pool\n",
    "x_train_labeled, x_pool, y_train_labeled, y_pool = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to create a CNN model with MC Dropout\n",
    "def create_mc_dropout_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# 'anova_df' is the DataFrame with your ANOVA results\n",
    "anova_df['F Statistic'].plot(kind='bar')\n",
    "plt.title('ANOVA Results for Hyperparameters')\n",
    "plt.ylabel('F Statistic')\n",
    "plt.xlabel('Hyperparameters')\n",
    "plt.show()\n",
    "best_hp = [-1, -1, 1, 1, 1, -1, 1]\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Active Learning Parameters\n",
    "n_queries = 20  # Number of queries per iteration\n",
    "n_iterations = 15  # Total number of iterations\n",
    "\n",
    "# Split the test dataset into a pool for active learning\n",
    "x_pool, y_pool = x_full_test, y_full_test\n",
    "x_pool = x_pool / 255.0\n",
    "y_pool = to_categorical(y_pool, 10)\n",
    "\n",
    "# Main Active Learning Loop\n",
    "for iteration in range(n_iterations):\n",
    "    # Define and compile the model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss=CategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size=64, epochs=3, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "    print(f\"Iteration {iteration+1} - Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # If this is the last iteration, don't need to select new samples\n",
    "    if iteration == n_iterations - 1:\n",
    "        break\n",
    "\n",
    "    # Select the next batch of samples for active learning\n",
    "    indices = np.random.choice(range(len(x_pool)), n_queries, replace=False)\n",
    "    x_selected = x_pool[indices]\n",
    "    y_selected = y_pool[indices]\n",
    "\n",
    "    # Add these points to the training data\n",
    "    x_train = np.concatenate((x_train, x_selected))\n",
    "    y_train = np.concatenate((y_train, y_selected))\n",
    "\n",
    "    # Remove these points from the pool\n",
    "    x_pool = np.delete(x_pool, indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, indices, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
